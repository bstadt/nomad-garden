import LatexFormula from "../../components/latex";

export const meta = {title: 'AI Trades Space for Time',
    description: '"The trend to highly parallel processors is the indication we are feeling the upper saturation limit of the S curve for single-processor computers" - Richard Hamming',
    thumbnail: '',
    date: 'Oct 1, 2023',
    priority: 1,
    hidden: true,
}

# AI Trades Space for Time
Recently, I was on a walk with my friend and frequent collaborator [Hayden Helm](https://scholar.google.com/citations?user=zEzJrqsAAAAJ&hl=en).
He asked me if I could sum up the fundamental innovation behind the current wave of AI in one sentence.
After thinking for a moment, I replied:
> AI makes it easy for programmers to trade space for time.

In this post, I am going to justify this answer by answering two sub-questions:
1) In what way does AI make it easy to trade space for time?
2) What makes this innovation so fundamental?

## Trading Space for Time in Traditional Algorithms
In traditional algorithms, the cost of a computation is measured using its [time complexity](https://en.wikipedia.org/wiki/Time_complexity) and its [space complexity](https://en.wikipedia.org/wiki/Space_complexity).
Here, time complexity refers to how long the program runs, and space complexity refers to the amount of data storage the program requires.
Programmers often make use of the [space-time tradeoff](https://en.wikipedia.org/wiki/Space%E2%80%93time_tradeoff) to make their programs faster by relying more heavily on data storage.
A simple example of this is [memoization](https://en.wikipedia.org/wiki/Memoization), where a program stores the result of an expensive operation that is repeatedly called.

## Why Time and Space Complexity?
The practice of measuring a program using its run time and memory consumption is a result of the emphasis placed on the RAM model of computing.
[CLRS](https://en.wikipedia.org/wiki/Introduction_to_Algorithms), the most popular textbook for teaching college level algorithms, makes this emphasis explicit:

> "For most of this book, we shall assume a generic one-processor, random-access machine (RAM) model of computation...
> In the RAM model, instructions are executed one after another, with no concurrent operations."
- CLRS, page 23

However, it seems reasonable that a program could be made faster by distributing it over multiple processors on a parallel computer.
This is desirable, since parallel computing would provide an alternative mechanism to the space-time tradeoff for speeding up programs.
CLRS does eventually mention parallel computing, but only after 772 pages:
> "Although the computing community settled on the random-access machine model for serial computing early on in the history of computer science, no single model for parallel computing has gained a wide acceptance...
> Unfortunately, programming a shared-memory parallel computer directly using static threads is difficult and error prone.
> One reason is that dynamically partitioning the work among the threads so that each thread receives approximately the same load turns out to be a complicated undertaking.
> For any but the simplest of applications, the programmer must use complex communication protocols to implement a scheduler to load-balance the work."
- CLRS, pages 772-773

Because of the difficulty involved in managing and coordinating a large number of threads, CLRS assumes that a program will only ever be distributed over a small number of threads.
As such, an asymptotic measure of "thread complexity," akin to the traditional measures of time and space complexity, is never mentioned.

## What Would Thread Complexity Look Like?
Let's assume for a moment that we live in a world where we can effectively coordinate a massive number of threads.
Instead of the RAM model, this world uses the GPU model, where we assume we have access to a computer with a large number of processors that can execute instructions concurrently.
Under the GPU model, it makes sense to start measuring programs using three different kinds of complexity:
1) Time Complexity: the amount of time it takes to run a program
2) Memory Complexity: the amount of memory the program requires
3) Thread Complexity: The number of processors threads

To get a feel for working with thread complexity, let's consider the task of multiplying N numbers together, which we will call MULTIPLY-N.
Under the RAM model, MULTIPLY-N takes O(N) memory, O(1) thread, and achieves O(N) time.

Under the GPU model, we can use a parallel [divide and conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm) algorithm to compute the products of several pairs of numbers at once.
For example, to compute a\*b\*c\*d using the GPU model, we would first compute (a\*b)=y and (c\*d)=z in parallel, and then finish by computing y*z.
The GPU implementation of MULTIPLY-N requires O(N) memory and O(N) thread, and achieves O(log(N)) time.
In other words, there seems to be a thread-space-time tradeoff in the GPU model, just like the space-time tradeoff in the RAM model.
Moreover, the GPU implementation of MULTIPLY-N achieves an <em>exponential</em> speedup when compared to its RAM conter part.

This leaves us with the following observation:
**If we have a computer with sufficiently many threads, and an algorithm that can coordinate them, we may be able to use a thread-space-time tradeoff to achieve an exponential speedup.**

NVIDIA's most recent [8xH100](https://docs.nvidia.com/launchpad/ai/h100-mig/latest/h100-mig-gpu.html) GPU boxes ship with 116736 total CUDA cores, radically dwarfing the 14 cores in the Intel i7.
Moreover, several 8xH100s can be networked together if even more cores are required.

## AI is How You Program Radically Space Parallel Computers

# Why is this so fundamental?
For the sake of this post, I am going to argue that an innovation is fundamental if it meets the following two criteria:
1) The innovation provides humans a new way to interact with the laws of nature.
2) This new interaction yields an exponential improvement according to some practical metric.

## Trading Space for Time in Physics
Our modern understanding of the relationship between space and time in physics is primarily based on Albert Einstein's theory of relativity.
Luckily, Einstein was both a great thinker and a great communicator.
In chapter 4 of his book [Relativity](https://www.amazon.com/Relativity-Special-General-Theory-Physics/dp/048641714X), he lays out a simple explanation of the relationship between space and time.

> "Space is a three-dimensional continuum.
> By this, we mean that it is possible to describe the position of a point by means of three numbers (coordinates): x, y, and z...
> Similarly, the world of physical phenomena is four-dimensional in the space-time sense.
> It is composed of individual events, each of which is described by four numbers: three space coordinates x, y, z, and a time value t."

- Albert Einstein

Einstein's key insight was that space and time should not be thought of as fundamentally separate.
Together, they make up spacetime, the singular entity required to describe physical phenomena.

Einstein's spacetime comes equipped with a tradeoff between space and time: the [spacetime-interval](https://en.wikipedia.org/wiki/Spacetime#Spacetime_interval).
