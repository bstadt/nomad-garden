export const meta = {
    title: 'Etc',
    description: 'the stream',
    thumbnail: '',
    date: '',
    hidden: true,
}

# This post contains stream of consciousness writing.

## Sep 17 2023
 - HC
 - Startups fundamentally change when they transition from Seed to Series A
    - Team growth
    - Introduction of process
    - Decrease in experimentation
    - The role of the CEO
       - High growth handbook
        - "Realize your old patterns of work can no longer apply"
        - "While you may be an excellent programmer, if your team now has 50 or 500 engineers, it is unlikely that writing code is the best thing you can contribute as CEO."
        - "It may be painful, but to scale your organization and move ahead as CEO you will likely nee to let go of certain parts of your prior roles that you enjoyed or thought were important."
 - What is p(GPT4All moment)? What about p(GPT4All moment | founder market fit)? What about p(GPT4All moment | hacker)?
 - I suspect the skill I have with the highest z-score is 0-1 hacking.
  - I fucking love 0-1 hacking. I derive existential joy from the process of creating something fundamentally new.
    - This is critical for the positive feedback loop, the "manic addictive mode," that is required to 0-1 something beautiful.
  - I have the skill set to actually execute on all parts of my idea
    - My entire background (Botball, MLH, Neurodata, Rad AI, Nomic) is in time & resource constrained programming challenges
  - I am willing to move on to the next project once I get to 1
    - Once you get to 1, the "jump" is "broken," and the "manic addictive mode" ceases
  - Perhaps the best way for me to help Nomic is to do a bunch of 0-1 projects that rely on Nomic technology.


## Sep 16 2023
 - HC
 - Craving deep dish, maybe because of all the UChicago people at the [Patronus](https://www.patronus.ai/) event.
    - Only now realizing how hard it may be to get deep dish in NYC.
    - I need me a rebel pizzeria
 - Just pushed out the starting Nomic post
    - I'll likely add context to it over time
        - More on scouting locations
        - Getting information from the first person to break a gap
            - Benj talks about this in several storror videos
  - Removed the thumbnails from posts.
    - The MJ images felt gimmicky, it made clicking links slow
    - Im thinking of having the walks fade over time and having new walks trigger on mouseover
        - also have to fix the overflow on the caption div to prevent the canvas from jumping around
  - Picked up grapefruit by yoko ono today
    - Ive known about this book for a long time, and am realizing ive done a few of the compositions
        - There is one about touching a bunch of walls to really get a feel for them that ive certainly done, and could write at length about it
        - I think im going to start working through it and writing about the experience

## Sep 15 2023
 - HC
 - Blog Next steps
    - Bugs
        - clicking links seems slow sometimes
        - pages take some time to load
    - Features
        - Random art on landing
            - I like that it goes out of the box sometimes - change color based on if it makes it
                - There is a CLT thing here where you track every time the generation occurs and see how often it goes out of the box
                - Perhaps have some aspect of the color that evolves over time
                - Progressively disappearing scribbles in the background
                    - Prevents the case of a uniform limit
    - Posts
        - On Food: The Menu, Fullmetal Alchemist, Promised Neverland
        - The Menu - The Mess
        - Ex Machina - Automatic Art
        - Nomic's approach to AI Safety
        - Starting Nomic Movement Composition
    - Infra
        - Some posts are a series
        - It would be good to have a layout that supports that
        - The tabbing system in my IDE is poor
            - Spend time using morph


## Sep 4 2023
 - Read [videofusion](https://arxiv.org/pdf/2303.08320.pdf) over coffee this morning
    - The main idea is to factor the noise in the diffusion process into two parts: base noise and residual noise
    - Base noise captures the majority of the image content and style, while residual noise captures the motion in the video
    - Base noise is shared across consecutive frames in the video, residual noise is not
    - Their architecture consists of two models, a base diffusion model (~2B param) and a residual diffusion model (~500M param)
    - The base model can be pretrained, so a LORA'd backbone can be used
        - For best results, the paper suggests to tune the base and residual models jointly
    - Section 2.2 mentions a quantized auto encoder to autoregressive transformer approach to video generation. This is exactly the arch used by the meta [audiocraft](https://github.com/facebookresearch/audiocraft) models.
    - My intuition is that the decoupled architecture will provide a lot of opportunity for caching, which will be necessary if we want this to be live audioreactive
 - Finally got the math rendering on this site - it was harder than it neaded to be
    - Adding remark and rehype options to next.config.mjs did not work. perhaps related to [this](https://github.com/remarkjs/remark-math/issues/71) issue.
    - Anyway, got it to work bu building a react component that renders latex and importing it in post files

## Sep 3 2023
 - [Post](https://www.instagram.com/reel/CwpmeMnIemI/) discussion with this.xor.that
    - post getting hate for stealing source work
        - steelman that data use is ok
            - the algorithm is a tool, like a brush. in the same way a painter may study or reproduce the style of an old master, so can the programmer behind the algorithm
            - [herbie hancock vibes](https://www.youtube.com/watch?v=n6QsusDS_8A)
        - steelman data use is not ok
            - the issue of access - what constitutes "consumption" or "inspiration"
            - the issue of compensation - artists will lose compensation if their art becomes primarily computational
        - current best idea
            - attribution
                - at the very least track what art is used to train what algorithm.
                - make this data public
                - this way, once a fair compensation structure is determined, it can be applied
    - this.xor.that is interested in an audioreactive diffusion video for livecode
        - try to replecate the post, but have the transition occur on the beat
        - similar to [kaiber.ai](https://kaiber.ai/)
        - step 1: generate video
            - brief search in coffee shop gave me this mvp approach- use a controlnet, and feed in the edges of the previous frame to get video
            - will likely start with huggingface diffusers for offline approach, then switch for inference optimization
            - [videofusion](https://arxiv.org/pdf/2303.08320.pdf) seems promising, and has a hf diffusers [implementation](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video)
            - how hard will training a LORA be on this arch? would be a great way to fit to a particular vibe
                - there is a [blog post](https://huggingface.co/docs/diffusers/training/lora) on it, but it only contains a text-to-image example.
                - adapting to video should be possible
        - it would be very interesting to fine tune on [keifergr33n](https://www.youtube.com/watch?v=-rgjhBOOYGw) and [trappin in japan](https://www.youtube.com/watch?v=8JmJdUjCDBM)
            - its stylistically defined paired visual music data!

