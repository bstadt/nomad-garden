export const meta = {
    title: 'Etc',
    description: 'the stream',
    thumbnail: '',
    date: ''
}

# This post contains stream of consciousness writing.

## Sep 4 2023
 - Read [videofusion](https://arxiv.org/pdf/2303.08320.pdf) over coffee this morning
    - The main idea is to factor the noise in the diffusion process into two parts: base noise and residual noise
    - Base noise captures the majority of the image content and style, while residual noise captures the motion in the video
    - Base noise is shared across consecutive frames in the video, residual noise is not
    - Their architecture consists of two models, a base diffusion model (~2B param) and a residual diffusion model (~500M param)
    - The base model can be pretrained, so a LORA'd backbone can be used
        - For best results, the paper suggests to tune the base and residual models jointly
    - Section 2.2 mentions a quantized auto encoder to autoregressive transformer approach to video generation. This is exactly the arch used by the meta [audiocraft](https://github.com/facebookresearch/audiocraft) models.
    - My intuition is that the decoupled architecture will provide a lot of opportunity for caching, which will be necessary if we want this to be live audioreactive
 - Finally got the math rendering on this site - it was harder than it neaded to be
    - Adding remark and rehype options to next.config.mjs did not work. perhaps related to [this](https://github.com/remarkjs/remark-math/issues/71) issue.
    - Anyway, got it to work bu building a react component that renders latex and importing it in post files

## Sep 3 2023
 - [Post](https://www.instagram.com/reel/CwpmeMnIemI/) discussion with this.xor.that
    - post getting hate for stealing source work
        - steelman that data use is ok
            - the algorithm is a tool, like a brush. in the same way a painter may study or reproduce the style of an old master, so can the programmer behind the algorithm
            - [herbie hancock vibes](https://www.youtube.com/watch?v=n6QsusDS_8A)
        - steelman data use is not ok
            - the issue of access - what constitutes "consumption" or "inspiration"
            - the issue of compensation - artists will lose compensation if their art becomes primarily computational
        - current best idea
            - attribution
                - at the very least track what art is used to train what algorithm.
                - make this data public
                - this way, once a fair compensation structure is determined, it can be applied
    - this.xor.that is interested in an audioreactive diffusion video for livecode
        - try to replecate the post, but have the transition occur on the beat
        - similar to [kaiber.ai](https://kaiber.ai/)
        - step 1: generate video
            - brief search in coffee shop gave me this mvp approach- use a controlnet, and feed in the edges of the previous frame to get video
            - will likely start with huggingface diffusers for offline approach, then switch for inference optimization
            - [videofusion](https://arxiv.org/pdf/2303.08320.pdf) seems promising, and has a hf diffusers [implementation](https://huggingface.co/docs/diffusers/api/pipelines/text_to_video)
            - how hard will training a LORA be on this arch? would be a great way to fit to a particular vibe
                - there is a [blog post](https://huggingface.co/docs/diffusers/training/lora) on it, but it only contains a text-to-image example.
                - adapting to video should be possible
        - it would be very interesting to fine tune on [keifergr33n](https://www.youtube.com/watch?v=-rgjhBOOYGw) and [trappin in japan](https://www.youtube.com/watch?v=8JmJdUjCDBM)
            - its stylistically defined paired visual music data!

